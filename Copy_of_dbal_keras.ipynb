{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Copy of dbal_keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadrathore/Provisioning-SmartX-MicroBox/blob/master/Copy_of_dbal_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm19ubMin95Z"
      },
      "source": [
        "# Deep Bayesian Active Learning with Image Data\n",
        "\n",
        "This is an implementation of the paper Deep Bayesian Active Learning with Image Data using keras and modAL. [modAL](https://modal-python.readthedocs.io/en/latest/) is an active learning framework for Python3, designed with modularity, flexibility and extensibility in mind. Built on top of scikit-learn, it allows you to rapidly create active learning workflows with nearly complete freedom. What is more, you can easily replace parts with your custom built solutions, allowing you to design novel algorithms with ease.\n",
        "\n",
        "## Active Learning\n",
        "\n",
        "In this notebook, we are concerned with pool-based Active Learning. In this setting, we have a large amount of unlabelled data and a small initial labelled training set and we want to choose what data should be labelled next.\n",
        "\n",
        "To do so, there are several query strategies. In this notebook, we will be using uncertainty sampling: the data chosen to be annotated is the one that maximizes an uncertainty criterion (entropy, gini index, variation ratios ...). \n",
        "\n",
        "## Dropout-Based Bayesian Deep Neural Networks\n",
        "\n",
        "In this Notebook, we will select the data from the unlabelled pool that maximizes the uncertainty of our model. But the model we will be using will be a Bayesian Deep Neural Network. \n",
        "\n",
        "Unlike Traditional Deep Learning, where we are looking for the set of weights that maximizes the likelihood of the data (MLE), in bayesian deep learning we are looking for the posterior distribution over the weights and the prediction is then obtained by marginalizing out the weights. As a result, Bayesian models are less prone to overfitting. But unfortunately for big deep models, the posterior distribution is intractable, and we need approximations. \n",
        "\n",
        "In 2015, [Gal and Ghahramani](https://arxiv.org/pdf/1506.02142.pdf) showed that deep models with dropout layers can be viewed as a lightweight bayesian approximation. The prior and posterior distributions are simply Bernoulli distributions (0 or the learned value). And the predictions can be cheaply obtained at test time by performing Monte Carlo integrations with dropout layers activated.\n",
        "\n",
        "So in a nutshell, Dropout-based Bayesian Neural Nets are simply Neural Nets with Dropout layers activated at test time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGe20hSqn95d"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.regularizers import l2\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from modAL.models import ActiveLearner\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from modAL.models import ActiveLearner\n",
        "\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONrgK7DtpEhE",
        "outputId": "965f8941-8f2d-480c-9f18-1120599a87bf"
      },
      "source": [
        "pip install modAL"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting modAL\n",
            "  Downloading https://files.pythonhosted.org/packages/76/63/fe3eea804b180ef85b07d4e99cd932d2b0f79db91ff31391b1d465943aa1/modAL-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->modAL) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->modAL) (1.15.0)\n",
            "Installing collected packages: modAL\n",
            "Successfully installed modAL-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WvlzB2Qn95e"
      },
      "source": [
        "def create_keras_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (4, 4), activation='relu'))\n",
        "    model.add(Conv2D(32, (4, 4), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spKnvpiin95f"
      },
      "source": [
        "### create the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIUGaa2dn95f"
      },
      "source": [
        "classifier = KerasClassifier(create_keras_model)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd6BChSzn95f"
      },
      "source": [
        "### read training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkPyA8DHn95g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01072fd5-52b6-4760-9a00-f405568532ed"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf0fheVAn95g"
      },
      "source": [
        "### preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrnBC33vn95h"
      },
      "source": [
        "X_train = X_train.reshape(60000, 28, 28, 1).astype('float32') / 255.\n",
        "X_test = X_test.reshape(10000, 28, 28, 1).astype('float32') / 255.\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP_7NkAZn95h"
      },
      "source": [
        "### initial labelled data\n",
        "We initialize the labelled set with 20 balanced randomly sampled examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkZdCFapn95h"
      },
      "source": [
        "initial_idx = np.array([],dtype=np.int)\n",
        "for i in range(10):\n",
        "    idx = np.random.choice(np.where(y_train[:,i]==1)[0], size=2, replace=False)\n",
        "    initial_idx = np.concatenate((initial_idx, idx))\n",
        "\n",
        "X_initial = X_train[initial_idx]\n",
        "y_initial = y_train[initial_idx]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLfojEA2n95i"
      },
      "source": [
        "### initial unlabelled pool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYmGU7rQn95i"
      },
      "source": [
        "X_pool = np.delete(X_train, initial_idx, axis=0)\n",
        "y_pool = np.delete(y_train, initial_idx, axis=0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEfJrGB9n95i"
      },
      "source": [
        "## Query Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9amDMX5n95j"
      },
      "source": [
        "### Uniform\n",
        "All the acquisition function we will use will be compared to the uniform acquisition function $\\mathbb{U}_{[0,1]}$ which will be our baseline that we would like to beat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vtovo11n95j"
      },
      "source": [
        "def uniform(learner, X, n_instances=1):\n",
        "    query_idx = np.random.choice(range(len(X)), size=n_instances, replace=False)\n",
        "    return query_idx, X[query_idx]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4XPt4wFn95j"
      },
      "source": [
        "### Entropy\n",
        "Our first acquisition function is the entropy:\n",
        "$$ \\mathbb{H} = - \\sum_{c} p_c \\log(p_c)$$\n",
        "where $p_c$ is the probability predicted for class c. This is approximated by:\n",
        "\\begin{align}\n",
        "p_c &= \\frac{1}{T} \\sum_t p_{c}^{(t)} \n",
        "\\end{align}\n",
        "where $p_{c}^{t}$ is the probability predicted for class c at the t th feedforward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcEKNyg4n95k"
      },
      "source": [
        "def max_entropy(learner, X, n_instances=1, T=100):\n",
        "    random_subset = np.random.choice(X.shape[0], 2000, replace=False)\n",
        "    MC_output = K.function([learner.estimator.model.layers[0].input, K.learning_phase()],\n",
        "                           [learner.estimator.model.layers[-1].output])\n",
        "    learning_phase = True\n",
        "    MC_samples = [MC_output([X[random_subset], learning_phase])[0] for _ in range(T)]\n",
        "    MC_samples = np.array(MC_samples)  # [#samples x batch size x #classes]\n",
        "    expected_p = np.mean(MC_samples, axis=0)\n",
        "    acquisition = - np.sum(expected_p * np.log(expected_p + 1e-10), axis=-1)  # [batch size]\n",
        "    idx = (-acquisition).argsort()[:n_instances]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X[query_idx]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7xWEhH_n95l"
      },
      "source": [
        "### Variation Ratio\n",
        "the Variation ratio is computed according to:\n",
        "\\begin{align}\n",
        "\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04HCq0bbn95l"
      },
      "source": [
        "def var_ratio(learner, X, n_instances=1, T=100):\n",
        "    random_subset = np.random.choice(X.shape[0], 2000, replace=False)\n",
        "    MC_output = K.function([learner.estimator.model.layers[0].input, K.learning_phase()],\n",
        "                           [learner.estimator.model.layers[-1].output])\n",
        "    learning_phase = True\n",
        "    MC_samples = [MC_output([X[random_subset], learning_phase])[0] for _ in range(T)]\n",
        "    MC_samples = np.array(MC_samples)  # [#samples x batch size x #classes]\n",
        "    preds = np.argmax(a, axis=2)\n",
        "    mode, count = stats.mode(preds, axis=0)\n",
        "    acquisition = (1 - count / preds.shape[1]).reshape((-1,))\n",
        "    idx = (-acquisition).argsort()[:n_instances]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X[query_idx]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GNsIuTyn95m"
      },
      "source": [
        "def bald(learner, X, n_instances, T=100):\n",
        "    random_subset = np.random.choice(X.shape[0], 2000, replace=False)\n",
        "    MC_output = K.function([learner.estimator.model.layers[0].input, K.learning_phase()],\n",
        "                           [learner.estimator.model.layers[-1].output])\n",
        "    learning_phase = True\n",
        "    MC_samples = [MC_output([X[random_subset], learning_phase])[0] for _ in range(T)]\n",
        "    MC_samples = np.array(MC_samples)  # [#samples x batch size x #classes]\n",
        "    expected_entropy = - np.mean(np.sum(MC_samples * np.log(MC_samples + 1e-10), axis=-1), axis=0)  # [batch size]\n",
        "    expected_p = np.mean(MC_samples, axis=0)\n",
        "    entropy_expected_p = - np.sum(expected_p * np.log(expected_p + 1e-10), axis=-1)  # [batch size]\n",
        "    acquisition = entropy_expected_p - expected_entropy\n",
        "    idx = (-acquisition).argsort()[:n_instances]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X[query_idx]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUH8bNKHn95m"
      },
      "source": [
        "### Active Learning Procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkGqUzHVn95m"
      },
      "source": [
        "def active_learning_procedure(query_strategy,\n",
        "                              X_test,\n",
        "                              y_test,\n",
        "                              X_pool,\n",
        "                              y_pool,\n",
        "                              X_initial,\n",
        "                              y_initial,\n",
        "                              estimator,\n",
        "                              epochs=50,\n",
        "                              batch_size=128,\n",
        "                              n_queries=100,\n",
        "                              n_instances=10,\n",
        "                              verbose=0):\n",
        "    learner = ActiveLearner(estimator=estimator,\n",
        "                            X_training=X_initial,\n",
        "                            y_training=y_initial,\n",
        "                            query_strategy=query_strategy,\n",
        "                            verbose=verbose\n",
        "                           )\n",
        "    perf_hist = [learner.score(X_test, y_test, verbose=verbose)]\n",
        "    for index in range(n_queries):\n",
        "        query_idx, query_instance = learner.query(X_pool, n_instances)\n",
        "        learner.teach(X_pool[query_idx], y_pool[query_idx], epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
        "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
        "        model_accuracy = learner.score(X_test, y_test, verbose=0)\n",
        "        print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n",
        "        perf_hist.append(model_accuracy)\n",
        "    return perf_hist"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXPuAHSBn95n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "1a3cb382-83ba-4ef5-9ac7-f70c41ea0e67"
      },
      "source": [
        "estimator = KerasClassifier(create_keras_model)\n",
        "entropy_perf_hist = active_learning_procedure(max_entropy,\n",
        "                                              X_test,\n",
        "                                              y_test,\n",
        "                                              X_pool,\n",
        "                                              y_pool,\n",
        "                                              X_initial,\n",
        "                                              y_initial,\n",
        "                                              estimator,)\n",
        "np.save(\"keras_max_entropy.npy\", entropy_perf_hist)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b8c2329e1cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                               \u001b[0mX_initial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                               \u001b[0my_initial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                               estimator,)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keras_max_entropy.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_perf_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-1cd29bdb6808>\u001b[0m in \u001b[0;36mactive_learning_procedure\u001b[0;34m(query_strategy, X_test, y_test, X_pool, y_pool, X_initial, y_initial, estimator, epochs, batch_size, n_queries, n_instances, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mperf_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_queries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mquery_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mX_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/modAL/models/base.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, X_pool, *query_args, **query_kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mlabelled\u001b[0m \u001b[0mupon\u001b[0m \u001b[0mquery\u001b[0m \u001b[0msynthesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mquery_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mquery_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mquery_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-0fbb2238692d>\u001b[0m in \u001b[0;36mmax_entropy\u001b[0;34m(learner, X, n_instances, T)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrandom_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     MC_output = K.function([learner.estimator.model.layers[0].input, K.learning_phase()],\n\u001b[0;32m----> 4\u001b[0;31m                            [learner.estimator.model.layers[-1].output])\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlearning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mMC_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMC_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_phase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   4060\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4061\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4062\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4064\u001b[0m     \u001b[0mwrap_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_keras_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_graph_inputs_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# A Network does not create weights of its own, thus it is already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_validate_graph_inputs_and_outputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    684\u001b[0m                          \u001b[0;34m'must come from `tf.keras.Input`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                          \u001b[0;34m'Received: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                          ' (missing previous layer metadata).')\n\u001b[0m\u001b[1;32m    687\u001b[0m       \u001b[0;31m# Check that x is an input tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input tensors to a Functional must come from `tf.keras.Input`. Received: 0 (missing previous layer metadata)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxQC5cGln95n"
      },
      "source": [
        "estimator = KerasClassifier(create_keras_model)\n",
        "bald_perf_hist = active_learning_procedure(bald,\n",
        "                                           X_test,\n",
        "                                           y_test,\n",
        "                                           X_pool,\n",
        "                                           y_pool,\n",
        "                                           X_initial,\n",
        "                                           y_initial,\n",
        "                                           estimator,)\n",
        "np.save(\"keras_bald.npy\", bald_perf_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZbIk_Ipn95o"
      },
      "source": [
        "estimator = KerasClassifier(create_keras_model)\n",
        "var_ratio_perf_hist = active_learning_procedure(var_ratio,\n",
        "                                               X_test,\n",
        "                                               y_test,\n",
        "                                               X_pool,\n",
        "                                               y_pool,\n",
        "                                               X_initial,\n",
        "                                               y_initial,\n",
        "                                               estimator,)\n",
        "np.save(\"keras_bald.npy\", bald_perf_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN9t9o7jn95p"
      },
      "source": [
        "estimator = KerasClassifier(create_keras_model)\n",
        "uniform_perf_hist = active_learning_procedure(uniform,\n",
        "                                              X_test,\n",
        "                                              y_test,\n",
        "                                              X_pool,\n",
        "                                              y_pool,\n",
        "                                              X_initial,\n",
        "                                              y_initial,\n",
        "                                              estimator,)\n",
        "np.save(\"keras_uniform.npy\", uniform_perf_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMQN8NfAn95p"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "plt.plot(entropy_perf_hist, label=\"entropy\")\n",
        "plt.plot(bald_perf_hist, label=\"bald\")\n",
        "plt.plot(uniform_perf_hist, label=\"uniform\")\n",
        "plt.ylim([0.7,1])\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}